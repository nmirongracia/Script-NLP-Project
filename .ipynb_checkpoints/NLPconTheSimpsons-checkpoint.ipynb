{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP EN DIÁLOGOS DE \"THE SIMPSONS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proyecto TEXTO para la asignatura de Análisis de Datos No Estructurados\n",
    "\n",
    "#### Laura Gutiérrez y Natalia Mirón\n",
    "#### Mayo 2022\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías a cargar\n",
    "import numpy as np\n",
    "import re  # Para preprocesamiento\n",
    "import pandas as pd  # Para manipulación de datos\n",
    "from time import time  # Para medir la duración de las operaciones\n",
    "from collections import defaultdict  # Para la frecuencia de las palabras\n",
    "\n",
    "import spacy  # Para preprocesamiento\n",
    "from datetime import datetime\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "from nltk import Text\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as  sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tf-nightly --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para mostrar columnas y filas\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option(\"display.max_rows\", 999)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# The Simpsons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar y Limpiar el Dataset\n",
    " El dataset contiene diálogos de más de 600 episodios de la serie \"The Simpsons\" en inglés, desde el 1989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el Dataset\n",
    "df = pd.read_csv('./data/simpsons_script_lines.csv')\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos el personaje y su speech\n",
    "df = df[[\"raw_character_text\",\"spoken_words\"]]\n",
    "df.columns = [\"name\", \"spoken_words\"] # cambiamos el nombre a las columnas\n",
    "print(\"El tamaño del datset es de: \" + str(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar Dataset\n",
    "# Eliminar valores nulos\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "print(\"\\nEl tamaño del dataset habiendo quitado los valores nulos es de: \" + str(df.shape))\n",
    "# Mostrar el Dataset\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Los personajes más \"Habladores\" de la serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Top personajes con más palabras\n",
    "topCharacters = df.groupby(\"name\",as_index=False).count().sort_values(\"spoken_words\", ascending=False).reset_index(drop=True)\n",
    "topCharactersNames = topCharacters.head(20).name.tolist()\n",
    "display(topCharacters.head(10))\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "sns.set(font_scale=1.3)  \n",
    "sns.barplot(x='spoken_words', y='name', data=topCharacters.head(10), color = \"salmon\")         \n",
    "\n",
    "plt.title(\"Top 10 de Personajes más habladores\", size=15)\n",
    "plt.xlabel('Palabras habladas', fontweight='bold', horizontalalignment='center')\n",
    "plt.ylabel('Nombre', fontweight='bold', horizontalalignment='center')\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(20,6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Preprocesamiento del texto\n",
    "* Eliminar los caracteres no-alfabéticos\n",
    "* Eliminar stop words\n",
    "* Aplicar técnica de lemitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Eliminar los caracteres no-alfabéticos:\n",
    "df.spoken_words = [re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['spoken_words']]\n",
    "df.spoken_words= [re.sub(\"[/']\", '', str(row)).lower() for row in df.spoken_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "    \n",
    "import functools\n",
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Quitamos las \"stop words\" del inglés\n",
    "stopwords = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def processSentence(sentence):\n",
    "    ww = sentence.split()\n",
    "    ww = [x for x in ww if x not in stopwords]\n",
    "    ww = [x for x in ww if x !=\"\"]\n",
    "    ww = [lemmatizer.lemmatize(x) for x in ww]\n",
    "    \n",
    "    if len(ww)>2:\n",
    "        return \" \".join(ww) \n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "cleaned = [processSentence(x) for x in df.spoken_words.tolist()]\n",
    "\n",
    "dfCleaned = df.copy()\n",
    "dfCleaned.spoken_words = cleaned\n",
    "dfCleaned = dfCleaned.dropna()\n",
    "dfCleaned = dfCleaned[~dfCleaned.spoken_words.duplicated()]\n",
    "dfCleaned.columns = [\"Name\", \"Sentence\"]\n",
    "\n",
    "wordsDf = pd.DataFrame(cleaned, columns=[\"sentence\"]).dropna().drop_duplicates()\n",
    "print(\"Conteo de palabras después del procesamiento: \"+ str(len(wordsDf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ranking de las palabras más Comunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [x.split() for x in wordsDf.sentence.tolist()]\n",
    "words = functools.reduce(operator.iconcat, words, [])\n",
    "wordCountsDF = pd.DataFrame(pd.Series(words).value_counts(), columns=[\"Count\"])\n",
    "wordCountsDF[\"Word\"] = wordCountsDF.index\n",
    "wordCountsDF = wordCountsDF[[\"Word\", \"Count\"]].reset_index(drop=True)\n",
    "display(wordCountsDF.head(10))\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.set(font_scale=1.3)  \n",
    "sns.barplot(x='Count', y='Word', data=wordCountsDF.head(20), color=\"salmon\")            \n",
    "plt.title(\"Ranking Palabras más Comunes\", size=15)\n",
    "plt.xlabel('Frecuencia', fontweight='bold', horizontalalignment='center')\n",
    "plt.ylabel('Palabra', fontweight='bold', horizontalalignment='center')\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(20,6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Word cloud\n",
    "\n",
    "Creación de un wordplot con las palabras más comunes habladas en la serie - en forma de BART.\n",
    "\n",
    "La palabra en la posición 14 es \"homer\" y en la posición 17 es \"bart\".\n",
    "\n",
    "La palabra más común (im) está en mayor tamaño, y el tamaño se reduce con las palabras menos frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "stopwords = list(set(STOPWORDS))\n",
    "\n",
    "\n",
    "def show_wordcloud(data, photo_path):\n",
    "    mask = np.array(Image.open(photo_path))\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=200,\n",
    "        max_font_size=50, \n",
    "        scale=5,\n",
    "        random_state=1, # chosen at random by flipping a coin; it was heads\n",
    "        mask=mask,\n",
    "        mode=\"RGBA\",\n",
    "        # contour_color='#023075',contour_width=3,colormap='rainbow'\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(16, 10))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.show()\n",
    "\n",
    "photo_path = \"./data/bart.png\"\n",
    "show_wordcloud(\" \".join(words), photo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gráfico de Dispersión\n",
    "\n",
    "Aquí miramos a palabras comunes y vemos su distribución a lo largo de los episodios.\n",
    "\n",
    "La lista de palabras es [\"homer\", \"simpson\", \"marge\", \"bart\", \"lisa\"]\n",
    "\n",
    "Se aprecia que todas estas palabras se distribuyen de forma uniforme a lo largo de todos los diálogos (de los 600 episodios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt= wordCountsDF.Word.tolist()\n",
    "\n",
    "tt = [\"homer\", \"simpson\", \"marge\", \"bart\", \"lisa\"]\n",
    "\n",
    "wordsTemp = [x for x in words if x in tt]\n",
    "tempdf = pd.DataFrame(wordsTemp, columns=[\"Word\"])\n",
    "tempdf[\"Ind\"] = tempdf.index\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "wordsTemp = [x for x in words if x in tt]\n",
    "sns.set(font_scale=1.3)  \n",
    "sns.stripplot(x=\"Ind\", y=\"Word\", data=tempdf, color=\"salmon\", alpha=0.3)            \n",
    "plt.title(\"Gráfico de Dispersión\", size=15)\n",
    "plt.xlabel('Índice de episodio', fontweight='bold', horizontalalignment='center')\n",
    "plt.ylabel('Palabra', fontweight='bold', horizontalalignment='center')\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(20,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Ranking de las Palabras Más Importantes por Personaje\n",
    "\n",
    "Se usa la técnica de TF-IDF (es una medida numérica que expresa cuán relevante es una palabra para un documento en una colección)\n",
    "\n",
    "Aplicamos la técnica en **Bart Simpson**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usaremos CountVectorizer para convertir una colección de documentos de texto a una matriz de conteo de tokens. \n",
    "# TfidfTransformers hace la transfromación de una matriz a un TF normalizado o representación TF-IDF.\n",
    "\n",
    "# Usamos 4 parámetros en el método CountVectorizer. Primero, stop_words elimina las stop words (aquellas palabras repetidas que no contienen información relevante). \n",
    "# En Scikit-learn, la lista de palabras stop words en inglés está incluida. \n",
    "# min_df es el límite mínimo para ignorar palabras con una frecuencia menor a min_df. \n",
    "# max_df es el límite superior, si la frequencia de una palabra en un documento es mayor a ese límite se ignora. \n",
    "# ngram_range(x,y) es el último parámetro. Define los límites de valores de n para los n-grams, que van del mñinimo (x) al máximo (y). \n",
    "# fit_transform devuelve la versión transformada de las frases.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "txt1 = [x for x in cleaned if str(x)!=\"nan\"]\n",
    "txt1 = dfCleaned[dfCleaned.Name==\"Bart Simpson\"].Sentence.tolist()\n",
    "\n",
    "cvec = CountVectorizer(stop_words=\"english\", analyzer='word', ngram_range=(1, 2), max_df=1.0, min_df=1)\n",
    "sf = cvec.fit_transform(txt1)\n",
    "    \n",
    "transformer = TfidfTransformer()\n",
    "transformed_weights = transformer.fit_transform(sf)\n",
    "weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': cvec.get_feature_names(), 'weight': weights})\n",
    "weights_df = weights_df.sort_values(\"weight\", ascending=False).reset_index(drop=False)\n",
    "\n",
    "sns.barplot(x='term', y='weight', data=weights_df.head(10))            \n",
    "plt.title(\"ITf IDF por token - Bart Simpson\")\n",
    "plt.xlabel('Peso', fontweight='bold', horizontalalignment='center')\n",
    "plt.ylabel('Palabra', fontweight='bold', horizontalalignment='center')\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(20, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que la palabra más relevante para Bart es 'dad'.\n",
    "\n",
    "Probamos esta técnica en otro personaje de la serie. \n",
    "\n",
    "Se analizarán las palabras más relevantes para el vecino de los Simpsons, Ned Flanders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1 = [x for x in cleaned if str(x)!=\"nan\"]\n",
    "txt1 = dfCleaned[dfCleaned.Name==\"Ned Flanders\"].Sentence.tolist()\n",
    "\n",
    "cvec = CountVectorizer(stop_words=\"english\", analyzer='word', ngram_range=(1, 2), max_df=1.0, min_df=1)\n",
    "sf = cvec.fit_transform(txt1)\n",
    "    \n",
    "transformer = TfidfTransformer()\n",
    "transformed_weights = transformer.fit_transform(sf)\n",
    "weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': cvec.get_feature_names(), 'weight': weights})\n",
    "weights_df = weights_df.sort_values(\"weight\", ascending=False).reset_index(drop=False)\n",
    "\n",
    "sns.barplot(x='term', y='weight', data=weights_df.head(10))            \n",
    "plt.title(\"ITf IDF por token - Ned Flanders\")\n",
    "plt.xlabel('Peso', fontweight='bold', horizontalalignment='center')\n",
    "plt.ylabel('Palabra', fontweight='bold', horizontalalignment='center')\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(20, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para Ned Flanders la palabra más relevante es \"homer\", esta palabra hace referencia a la misma persona que en el caso de Bart Simpson. \n",
    "\n",
    "En los dos casos, la palabra más relevante hace referencia a Homer Simpson.\n",
    "\n",
    "Esto puede resultar interesante, y se podría analizar como cada personaje llama a la misma persona. Ejemplo: Dad, Homer, Mr. Simpson...\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relaciones entre palabras\n",
    "\n",
    "Se han explorado las palabras que son más importantes para un personaje. Ahora, se explorarán las relaciones entre palabras.\n",
    "\n",
    "Se usará la técnica de NLP de n-Grams!\n",
    "\n",
    "\n",
    "Convertimos los datos a nGrams de 2 palabras\n",
    "* N-grams (sets of consecutive words)\n",
    "* N=2\n",
    "\n",
    "### 6. Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "ngrams = list([nltk.ngrams(words, 2)])\n",
    "\n",
    "# Lista flatten list de bigrams\n",
    "bigrams = list(itertools.chain(*ngrams))\n",
    "\n",
    "# Crear un conteo de palabras en los bigrams\n",
    "bigram_counts = collections.Counter(bigrams)\n",
    "bigram_df = pd.DataFrame(bigram_counts.most_common(30), columns=['bigram', 'count'])\n",
    "\n",
    "display(bigram_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los bigrams, se observan combinaciones entre palabras habituales. Por ejemplo, el tercer bigram más común es \"im, sorry\", una expresión común.\n",
    "\n",
    "El bigram más común entre los episodios de \"The Simpsons\" es \"im, gonna\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualizar el network de Bigrams\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "def visualiseBigrams(bigram_df, K):\n",
    "    d = bigram_df.set_index('bigram').T.to_dict('records')\n",
    "\n",
    "    # Crear el network plot \n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Crear las conexiones entre nodos\n",
    "    for k, v in d[0].items():\n",
    "        G.add_edge(k[0], k[1], weight=(v * 10))\n",
    "\n",
    "    # G.add_node(\"china\", weight=100)\n",
    "    fig, ax = plt.subplots(figsize=(20, 8))\n",
    "    \n",
    "    # k : float (default=None)\n",
    "    # Distancia óptima entre nodos. \n",
    "    # Si es 'None' la distancia es 1/sqrt(n) donde n es el número de nodos. Al aumentar este valor la distancia entre nodos aumenta.\n",
    "    pos = nx.spring_layout(G, k=K)\n",
    "\n",
    "    # Representar los networks gráficamente\n",
    "    nx.draw_networkx(G, pos,\n",
    "                     font_size=13,\n",
    "                     width=2,\n",
    "                     edge_color='grey',\n",
    "                     node_color='pink',\n",
    "                     with_labels = False,\n",
    "                     ax=ax)\n",
    "\n",
    "    # Crear las etiquetas de offset\n",
    "    for key, value in pos.items():\n",
    "        x, y = value[0]+.02, value[1]+.045\n",
    "        ax.text(x, y,\n",
    "                s=key,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.25),\n",
    "                horizontalalignment='center', fontsize=13)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "visualiseBigrams(bigram_df=bigram_df,  K=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La gráfica de los bigrams de arriba muestra palabras que deberían seguir a otras.\n",
    "\n",
    "Se aprecia que la palabra \"im\" debe ser precedida por palabras como:\n",
    " * sure\n",
    " * afraid\n",
    " * gonna\n",
    " * sorry\n",
    " * well\n",
    " * going\n",
    " \n",
    "Analizando los distintos bigrams y las palabras que preceden la palabra base, sí parece tener sentido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el gráfico de \"network\" para una palabra en específico\n",
    "\n",
    "Elegimos la palabra \"simpson\", y sacamos las 20 palabras más comunes que siguen a \"simpson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gráfico de una palabra en específica\n",
    "\n",
    "wordGToGraph = \"simpson\"\n",
    "bigram_df = pd.DataFrame(bigram_counts.most_common(20), columns=['bigram', 'count'])\n",
    "\n",
    "### filter nframs list\n",
    "bigram_counts_filter = []\n",
    "for k in bigram_counts:\n",
    "    #print(k)\n",
    "    if wordGToGraph in k:\n",
    "        bigram_counts_filter.append(k)\n",
    "        \n",
    "bigram_counts_filter = collections.Counter(bigram_counts_filter)\n",
    "bigram_df_filter = pd.DataFrame(bigram_counts_filter.most_common(20), columns=['bigram', 'count'])\n",
    "visualiseBigrams(bigram_df=bigram_df_filter,  K=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams con n = 3\n",
    "\n",
    "En este caso, es más difícil/útil sacar n-grams con 3 palabras relacionadas.\n",
    "\n",
    "Como comentario, aparece 81 veces las 3 palabras juntas: (\"kwik\",\"e\", \"mart\"), que hace referencia al \"badulake\" que es el supermercado donde suele acudir la familia Simpson y 60 veces aparece: (\"santa\",\"little\", \"helper\"), que se refiere al perro de la familia: \"pequeño ayudante de Santa Claus\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ngrams = list([nltk.ngrams(words, 3)])\n",
    "\n",
    "# Lista flatten list de bigrams\n",
    "bigrams = list(itertools.chain(*ngrams))\n",
    "\n",
    "# Crear un conteo de palabras en los bigrams\n",
    "bigram_counts = collections.Counter(bigrams)\n",
    "bigram_df = pd.DataFrame(bigram_counts.most_common(30), columns=['bigram', 'count'])\n",
    "\n",
    "display(bigram_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 7. Sentiment analysis por cada personaje\n",
    "\n",
    "Para el séptimo apartado del proyecto, se va a analizar el \"sentimiento\" de los personajes principales. Esta práctica es muy común y separará a los personajes más alegres y menos, dependiendo de su manera de hablar y las palabras usadas en los diálogos.\n",
    "\n",
    "Para hacer esta clasificación, se recurre a la librería TextBlob que devuelve la ppolaridad y la subjetividad de una frase. A cada línea de diálogo se le asigna una puntuación de 1 (sentimiento positivo) a -1 (sentimiento negativo) y sacando el valor medio de las líneas agrupando por personaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "### Calcular sentimiento para todos los datos\n",
    "dfCleaned[\"Sentiment\"] = [TextBlob(x).sentiment.polarity for x in dfCleaned.Sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seleccionamos un subset, son los 70 primeros personajes que aparecen\n",
    "topCharactersNames = topCharacters.head(70).name.tolist()\n",
    "dfCleanedTopCharacters = dfCleaned[dfCleaned.Name.isin(topCharactersNames)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentimientos más negativos (Top 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### top sentimientos menos positivos: (head)\n",
    "dfCleanedTopCharacters.sort_values(\"Sentiment\").head().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentimientos más positivos (Top 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### top sentimientos más positivos: (tail)\n",
    "dfCleanedTopCharacters.sort_values(\"Sentiment\").tail().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sentimiento por personaje\n",
    "sentimentByName = dfCleanedTopCharacters[dfCleanedTopCharacters.Sentiment.abs()>=0.05].groupby(\"Name\", as_index=False).mean()\n",
    "sentimentByName = sentimentByName.sort_values(\"Sentiment\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación de los Sentimientos de 20 de los peronsajes con mejor y peor puntuación de Sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "tempSentiment = sentimentByName.head(10)\n",
    "tempSentiment = tempSentiment.append(sentimentByName.tail(10)).reset_index(drop=True)\n",
    "\n",
    "sns.barplot(x='Sentiment', y='Name', data=tempSentiment, color=\"salmon\")            \n",
    "plt.title(\"Sentiment Analysis - Personajes\", size=20)\n",
    "plt.xlabel('Valor de Sentimiento', fontweight='bold', horizontalalignment='center')\n",
    "plt.ylabel('Nombre', fontweight='bold', horizontalalignment='center')\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(20,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La persona con el sentimiento más positivo es \"Miss Hoover\", seguido de \"Adult Bart\"\n",
    "\n",
    "Las personas más negativas en sentimiento son \"Maude Flanders\" y \"Woman\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Representación de los Sentimientos de los Protagonistas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparar los sentimientos de los protagonistas:\n",
    "filtro = sentimentByName[sentimentByName['Name'].str.contains(\"Simpson\", case=False)]\n",
    "filtro = filtro.sort_values(\"Sentiment\", ascending=False).reset_index(drop=True)\n",
    "filtro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.barplot(x='Sentiment', y='Name', data=filtro, color=\"salmon\")            \n",
    "plt.title(\"Sentiment Analysis - Personajes Principales\", size=20)\n",
    "plt.xlabel('Valor de Sentimiento', fontweight='bold', horizontalalignment='center')\n",
    "plt.ylabel('Nombre', fontweight='bold', horizontalalignment='center')\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(20,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los protagonistas de la serie observamos que\n",
    "\n",
    "* Marge Simpson tiene el sentimiento más positivo, aunque es bajo (0.139).\n",
    "* El abuelo de los Simpson es el protagonista con la menor puntuación (menos positivo).\n",
    "* Los protagonistas tienen todos un sentimiento positivio pero prácticamente neutral.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### 8. Rolling Sentiment - Sentimiento a lo largo de los episodios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparamos la puntuación del Sentimiento a lo largo de los episodios\n",
    "def rollingSentiment(data, character, window, k):\n",
    "    if character!=\"\":\n",
    "        temp = data[data.Name==character].reset_index(drop=True)\n",
    "    else:\n",
    "        temp = data.copy()\n",
    "    temp = temp[temp.Sentiment.abs()>=k].reset_index(drop=True)\n",
    "    movingS = temp.rolling(window).mean().dropna()\n",
    "    movingS[\"Time\"] = movingS.index\n",
    "    \n",
    "    sns.set(style=\"whitegrid\")\n",
    "    sns.set(font_scale=1.3)  \n",
    "    sns.lineplot(x=\"Time\", y='Sentiment', data=movingS, color=\"b\")            \n",
    "    plt.title(\"Sentimiento para \" + character, size=20)\n",
    "    plt.xlabel('Tiempo', fontweight='bold', horizontalalignment='center')\n",
    "    plt.ylabel('Valor de Sentimiento', fontweight='bold', horizontalalignment='center')\n",
    "    plt.ylim(-0.1,0.3)\n",
    "    # plt.xlim(0,400)\n",
    "    fig=plt.gcf()\n",
    "    fig.set_size_inches(20, 6)\n",
    "    plt.show()\n",
    "    return movingS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolData = rollingSentiment(data=dfCleaned, character=\"Milhouse Van Houten\", window=50, k=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolData = rollingSentiment(data=dfCleaned, character=\"Marge Simpson\", window=500, k=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolData = rollingSentiment(data=dfCleaned, character=\"Krusty the Clown\", window=100, k=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolData = rollingSentiment(data=dfCleaned, character=\"Homer Simpson\", window=500, k=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolData = rollingSentiment(data=dfCleaned, character=\"Dr. Julius Hibbert\", window=50, k=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al representarlo con los mismos límites en el eje y, se puede apreciar que cada personaje evoluciona de una manera diferente siendo el Dr Hibbert el que parece tener un sentimiento más negativo a lo largo de los episodios alcanzando el pico mínimo en el intervalo [150, 180]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 9. Word Embedding - Word2Vec\n",
    "\n",
    "### Training the model\n",
    "#### Implementación del modelo de Gensim Word2Vec :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando la función de procesamiento definida al principio se realiza el preprocesamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned2 = [processSentence(x) for x in df.spoken_words.tolist()]\n",
    "words22 = [x for x in cleaned2 if str(x)!=\"nan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Cuenta los cores del ordenador\n",
    "words33 = [x.split() for x in words22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=5,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1,\n",
    "                     sg = 1)\n",
    "\n",
    "w2v_model.build_vocab(words33, progress_per=10000)\n",
    "\n",
    "vocab_size = len(w2v_model.wv.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = datetime.now()\n",
    "\n",
    "w2v_model.train(words33, total_examples=w2v_model.corpus_count, epochs=50, report_delay=1)\n",
    "\n",
    "print('Tiempo de entrenamiento: ' +  str(datetime.now()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True) # para normalizar los vectores de word2vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Explorar el modelo\n",
    "\n",
    "#### Palabras más similares a otras:\n",
    "\n",
    "Aquí, pedimos a nuestro modelo que encuentre las palabras más similares a los protagnistas de \"The Simpsons\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Empezamos con Homer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"homer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La palabra más similar a homer es \"simpson\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seguimos con Simpson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"simpson\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Marge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"marge\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"bart\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similitudes:\n",
    "\n",
    "Aquí, comparamos dos palabras para ver como son de similares entre ellas\n",
    "\n",
    "Primero, comparamos a \"maggie\" con \"baby\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('maggie', 'baby')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora a \"homer\" con \"dad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('homer', 'dad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diferencia de analogía:\n",
    "\n",
    "¿Qué palabra es para \"woman\" como \"homer\" es a \"man\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"homer\"], negative=[\"man\"], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relaciones entre los nombres de los personajes\n",
    "\n",
    "Creamos un modelo de PCA en 2-dimensiones de los vectores de palabras usandola clase de PCA de scikit-learn.\n",
    "\n",
    "Usamos los primeros 20 personajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Viz characters\n",
    "characters = topCharacters.head(20).name.tolist()\n",
    "characters = [re.sub(\"[^A-Za-z.']+\", ' ', str(row)).lower() for row in characters]\n",
    "characters = [re.sub(\"[/']\", '', str(row)) for row in characters]\n",
    "characters = sum([x.split() for x in characters], [])\n",
    "characters = np.unique([x for x in characters if len(x)>2 and \".\" not in x ])\n",
    "characters = [x for x in characters if x in w2v_model.wv.key_to_index.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = w2v_model.wv[characters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# crear un plot de scatter de la projección\n",
    "fig = plt.figure(figsize=(23,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(result[:, 0], result[:, 1])\n",
    "ax.set_title(\"Scatter Plot de la Proyección de PCA del Modelo Word2Vec\")\n",
    "for i, word in enumerate(characters):\n",
    "    ax.annotate(word, xy=(result[i, 0]+0.0001, result[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se podría comentar que en el lado derecho del gráfico aparecen los protagonistas de la famila Simpson (una clara agrupación)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 Palabras con Mayor similitud words vs. 8 Palabras Aleatorias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsnescatterplot(model, word, list_names):\n",
    "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n",
    "    its list of most similar words, and a list of words.\n",
    "    \"\"\"\n",
    "    \n",
    "    sns.set_style(\"darkgrid\")\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    arrays = np.empty((0, 300), dtype='f')\n",
    "    word_labels = [word]\n",
    "    color_list  = ['red']\n",
    "\n",
    "    # adds the vector of the query word\n",
    "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
    "    \n",
    "    # gets list of most similar words\n",
    "    close_words = model.wv.most_similar([word])\n",
    "    \n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "    \n",
    "    # adds the vector for each of the words from list_names to the array\n",
    "    for wrd in list_names:\n",
    "        wrd_vector = model.wv.__getitem__([wrd])\n",
    "        word_labels.append(wrd)\n",
    "        color_list.append('green')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "        \n",
    "    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n",
    "    reduc = PCA(n_components=10).fit_transform(arrays)\n",
    "    \n",
    "    # Finds t-SNE coordinates for 2 dimensions\n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
    "    \n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
    "                       'y': [y for y in Y[:, 1]],\n",
    "                       'words': word_labels,\n",
    "                       'color': color_list})\n",
    "    \n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(15, 7)\n",
    "    \n",
    "    # Basic plot\n",
    "    sns.set(font_scale=1.3)  \n",
    "    p1 = sns.regplot(data=df,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     fit_reg=False,\n",
    "                     marker=\"o\",\n",
    "                     scatter_kws={'s': 40,\n",
    "                                  'facecolors': df['color']\n",
    "                                 }\n",
    "                    )\n",
    "    \n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],\n",
    "                 df['y'][line],\n",
    "                 '  ' + df[\"words\"][line].title(),\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',\n",
    "                 color=df['color'][line],\n",
    "                 weight='normal'\n",
    "                ).set_size(15)\n",
    "\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "    plt.title('Visualización de t-SNE para {}'.format(word.title()), size=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo es representar el vector con altas dimensiones a un gráfico de 2 dimensiones.\n",
    "\n",
    "Para ello se usará la implementación de t-SNE de scikit-learn.\n",
    "\n",
    "La visualización se compone de:\n",
    "* la palabra de la query (en **rojo**)\n",
    "* las palabras más similares de la palabra de la query (en **azul**)\n",
    "* palabras aleatorias (en **verde**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnescatterplot(w2v_model, 'homer', ['dog', 'bird', 'ah', 'maude', 'bob', 'mel', 'apu', 'duff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa como las palabras más similares a \"homer\" aparecen más cerca (azules) que las aleatorias (verde) - esto era lo esperado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Con esto se acaba nuestro análisis NLP de los diálogos de la serie de The Simpsons!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
